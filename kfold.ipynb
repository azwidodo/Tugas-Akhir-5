{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from utils import *\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten, Conv1D, Dropout, MaxPooling1D, GlobalMaxPooling1D, LSTM, GRU, SimpleRNN\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import tensorflow.random as random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "seed = 8\n",
    "random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"glove.42B.300d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        token = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[token] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>detoken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This started out strong, but it went downhill ...</td>\n",
       "      <td>[start, out, strong, go, downhill, fairly, qui...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>start out strong go downhill fairly quickly no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A decently written YA book, but I can't even c...</td>\n",
       "      <td>[decently, write, ya, book, can, even, conside...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>decently write ya book can even consider end o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugh...I tried, I honestly tried. I'm a huge fa...</td>\n",
       "      <td>[ugh, try, honestly, try, huge, fan, scott, we...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ugh try honestly try huge fan scott westerfeld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I hate to give any book this low of a rating -...</td>\n",
       "      <td>[hate, give, any, book, low, rat, know, take, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>hate give any book low rat know take actually ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Main points: \\n 1. Never ever introduce a poin...</td>\n",
       "      <td>[main, point, never, ever, introduce, point, v...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>main point never ever introduce point view sid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  This started out strong, but it went downhill ...   \n",
       "1  A decently written YA book, but I can't even c...   \n",
       "2  Ugh...I tried, I honestly tried. I'm a huge fa...   \n",
       "3  I hate to give any book this low of a rating -...   \n",
       "4  Main points: \\n 1. Never ever introduce a poin...   \n",
       "\n",
       "                                                text  rating  sentiment  \\\n",
       "0  [start, out, strong, go, downhill, fairly, qui...       1          0   \n",
       "1  [decently, write, ya, book, can, even, conside...       1          0   \n",
       "2  [ugh, try, honestly, try, huge, fan, scott, we...       1          0   \n",
       "3  [hate, give, any, book, low, rat, know, take, ...       1          0   \n",
       "4  [main, point, never, ever, introduce, point, v...       1          0   \n",
       "\n",
       "                                             detoken  \n",
       "0  start out strong go downhill fairly quickly no...  \n",
       "1  decently write ya book can even consider end o...  \n",
       "2  ugh try honestly try huge fan scott westerfeld...  \n",
       "3  hate give any book low rat know take actually ...  \n",
       "4  main point never ever introduce point view sid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cleaned_data/cleaned_reviews_4.csv\")\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: ast.literal_eval(x))\n",
    "df[\"detoken\"] = df[\"text\"].apply(lambda y: TreebankWordDetokenizer().tokenize(y))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1218433 train words in total and the vocabulary size is 29796.\n",
      "There are 303603 test words in total and the vocabulary size is 15661.\n"
     ]
    }
   ],
   "source": [
    "df_pos, df_neg = split_sentiment(df)\n",
    "df_train, df_test = split_train_test(df_pos, df_neg, 333)\n",
    "\n",
    "train_words = [word for text in df_train[\"text\"] for word in text]\n",
    "train_text_length = [len(text) for text in df_train[\"text\"]]\n",
    "\n",
    "vocab = list(set(train_words))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "test_words = [word for text in df_test[\"text\"] for word in text]\n",
    "test_vocab_size = len(list(set(test_words)))\n",
    "\n",
    "print(f\"There are {len(train_words)} train words in total and the vocabulary size is {vocab_size}.\")\n",
    "print(f\"There are {len(test_words)} test words in total and the vocabulary size is {test_vocab_size}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df[\"detoken\"].tolist()\n",
    "targets = np.asarray(df[\"sentiment\"])\n",
    "\n",
    "MAX_WORDS = 20000\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(inputs)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "inputs_sequences = tokenizer.texts_to_sequences(inputs)\n",
    "\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "inputs_padded = pad_sequences(inputs_sequences, maxlen=MAX_LENGTH, padding=\"post\")\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "vocab_size = min(len(word_index) + 1, MAX_WORDS)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_WORDS:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        embedding_vector = embeddings_dict[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    except KeyError:\n",
    "        embedding_vector = np.zeros(EMBEDDING_DIM)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_LENGTH, trainable=False)\n",
    "\n",
    "del(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 0.7832042574882507; accuracy of 49.78571534156799%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 0.7770660519599915; accuracy of 50.428569316864014%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 0.7798271179199219; accuracy of 55.642855167388916%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 0.5693315267562866; accuracy of 73.21428656578064%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 0.7021685838699341; accuracy of 65.4285728931427%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 0.7152639031410217; accuracy of 53.42857241630554%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 0.6556516885757446; accuracy of 64.85714316368103%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 0.7503200769424438; accuracy of 53.214287757873535%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 0.6330263018608093; accuracy of 70.35714387893677%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 0.6184358596801758; accuracy of 70.85714340209961%\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs_padded, targets):\n",
    "\n",
    "  # Define the model architecture\n",
    "  model = Sequential()\n",
    "  model.add(embedding_layer)\n",
    "  model.add(SimpleRNN(32))\n",
    "  model.add(Dense(16, activation=\"relu\"))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  # Compile the model\n",
    "  EPOCHS = 5\n",
    "  LEARNING_RATE = 0.001\n",
    "  BATCH_SIZE = 64\n",
    "  optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "  model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  # Fit data to model\n",
    "  history = model.fit(inputs_padded[train], targets[train], batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=0, validation_split=0.3)\n",
    "\n",
    "  # Generate generalization metrics\n",
    "  scores = model.evaluate(inputs_padded[test], targets[test], verbose=0)\n",
    "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "  acc_per_fold.append(scores[1] * 100)\n",
    "  loss_per_fold.append(scores[0])\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.7832042574882507 - Accuracy: 49.78571534156799%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.7770660519599915 - Accuracy: 50.428569316864014%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.7798271179199219 - Accuracy: 55.642855167388916%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5693315267562866 - Accuracy: 73.21428656578064%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.7021685838699341 - Accuracy: 65.4285728931427%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 0.7152639031410217 - Accuracy: 53.42857241630554%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 0.6556516885757446 - Accuracy: 64.85714316368103%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 0.7503200769424438 - Accuracy: 53.214287757873535%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 0.6330263018608093 - Accuracy: 70.35714387893677%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 0.6184358596801758 - Accuracy: 70.85714340209961%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 60.721428990364075 (+- 8.670525737988408)\n",
      "> Loss: 0.698429536819458\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbdeae21cd698169d80b8f85757407d35b6d9920a79c4c6b4fbd15284371fc7c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('final')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
