{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "from string import digits, punctuation\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Dense, Flatten, Conv1D, Dropout, MaxPooling1D, Activation, GlobalMaxPooling1D, Input, Conv2D, Reshape, MaxPooling2D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data by sentiment\n",
    "def split_sentiment(df):\n",
    "\n",
    "    df_pos = df[df[\"sentiment\"] == 1]\n",
    "    df_neg = df[df[\"sentiment\"] == 0]\n",
    "\n",
    "    return df_pos, df_neg\n",
    "\n",
    "\n",
    "# Split training and testing data\n",
    "def split_train_test(df_pos, df_neg, random_state=None):\n",
    "\n",
    "    df_train_pos, df_test_pos = train_test_split(df_pos, test_size=0.2, random_state=random_state)\n",
    "    df_train_neg, df_test_neg = train_test_split(df_neg, test_size=0.2, random_state=random_state)\n",
    "\n",
    "    df_train = pd.concat([df_train_pos, df_train_neg])\n",
    "    df_test = pd.concat([df_test_pos, df_test_neg])\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    text = url.sub(r\"\", text)\n",
    "    \n",
    "    rem_digits = str.maketrans(digits, \" \"*len(digits))\n",
    "    text = text.translate(rem_digits)\n",
    "    \n",
    "    rem_punctuation = str.maketrans(punctuation, \" \"*len(punctuation))\n",
    "    text = text.translate(rem_punctuation)\n",
    "    text = re.sub(r'[^\\w\\s]',' ', text)\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word, pos=\"v\") for word in tokens]\n",
    "    \n",
    "    tokens = [word for word in tokens if not word in stopwords_set]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>token</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[one, reviewers, mention, watch, oz, episode, ...</td>\n",
       "      <td>one reviewers mention watch oz episode hook ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "      <td>[wonderful, little, production, br, br, film, ...</td>\n",
       "      <td>wonderful little production br br film techniq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>[think, wonderful, way, spend, time, hot, summ...</td>\n",
       "      <td>think wonderful way spend time hot summer week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[basically, family, little, boy, jake, think, ...</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "      <td>[petter, mattei, love, time, money, visually, ...</td>\n",
       "      <td>petter mattei love time money visually stun fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...          1   \n",
       "1  A wonderful little production. <br /><br />The...          1   \n",
       "2  I thought this was a wonderful way to spend ti...          1   \n",
       "3  Basically there's a family where a little boy ...          0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1   \n",
       "\n",
       "                                               token  \\\n",
       "0  [one, reviewers, mention, watch, oz, episode, ...   \n",
       "1  [wonderful, little, production, br, br, film, ...   \n",
       "2  [think, wonderful, way, spend, time, hot, summ...   \n",
       "3  [basically, family, little, boy, jake, think, ...   \n",
       "4  [petter, mattei, love, time, money, visually, ...   \n",
       "\n",
       "                                               clean  \n",
       "0  one reviewers mention watch oz episode hook ri...  \n",
       "1  wonderful little production br br film techniq...  \n",
       "2  think wonderful way spend time hot summer week...  \n",
       "3  basically family little boy jake think zombie ...  \n",
       "4  petter mattei love time money visually stun fi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df[\"token\"] = df[\"review\"].apply(lambda x: preprocess(x))\n",
    "df[\"sentiment\"] = df[\"sentiment\"].apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "df[\"clean\"] = df[\"token\"].apply(lambda x: \" \".join(x))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x and y for training and testing\n",
    "def create_xy(df_train, df_test):\n",
    "    cv = CountVectorizer().fit(df_train[\"clean\"])\n",
    "\n",
    "    x_train = cv.transform(df_train[\"clean\"]).toarray()\n",
    "    x_test = cv.transform(df_test[\"clean\"]).toarray()\n",
    "\n",
    "    y_train = np.array(df_train[\"sentiment\"])\n",
    "    y_test = np.array(df_test[\"sentiment\"])\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Feature counts and log probabilities\n",
    "def feature_log_probs(x_train, y_train):\n",
    "    y = LabelBinarizer().fit_transform(y_train)\n",
    "\n",
    "    if y.shape[1] == 1:\n",
    "        y = np.concatenate((1 - y, y), axis=1)\n",
    "\n",
    "    fc = np.matmul(y.T, x_train)\n",
    "\n",
    "    smoothed_fc = fc + 1\n",
    "    smoothed_cc = smoothed_fc.sum(axis=1)\n",
    "    log_probs = np.log(smoothed_fc) - np.log(smoothed_cc.reshape(-1, 1))\n",
    "\n",
    "    return log_probs\n",
    "\n",
    "\n",
    "# Predict using posterior probabilities\n",
    "def predict(x_test, log_probs):\n",
    "    posterior = np.matmul(x_test, log_probs.T)\n",
    "    prediction = np.argmax(posterior, axis=1)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes Model: 0.8524\n"
     ]
    }
   ],
   "source": [
    "df_pos, df_neg = split_sentiment(df)\n",
    "df_train, df_test = split_train_test(df_pos, df_neg, 222)\n",
    "\n",
    "x_train, x_test, y_train, y_test = create_xy(df_train, df_test)\n",
    "log_probs = feature_log_probs(x_train, y_train)\n",
    "\n",
    "y_predict = predict(x_test, log_probs)\n",
    "accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "\n",
    "print(f\"Accuracy of Naive Bayes Model: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: 40000\n",
      "Size of testing data: 10000\n"
     ]
    }
   ],
   "source": [
    "# Training and testing data\n",
    "\n",
    "print(f\"Size of training data: {len(df_train)}\")\n",
    "print(f\"Size of testing data: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4879067 train words in total and the vocabulary size is 78508.\n"
     ]
    }
   ],
   "source": [
    "# Create vocab and its length\n",
    "\n",
    "train_words = [word for text in df_train[\"token\"] for word in text]\n",
    "train_text_length = [len(text) for text in df_train[\"token\"]]\n",
    "\n",
    "vocab = list(set(train_words))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"There are {len(train_words)} train words in total and the vocabulary size is {vocab_size}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "(40000, 200)\n",
      "(10000, 200)\n",
      "(40000,)\n",
      "(10000,)\n",
      "Found 78508 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# Create word vectors\n",
    "\n",
    "train_data = df_train[\"clean\"].tolist()\n",
    "test_data = df_test[\"clean\"].tolist()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=30000)\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "# max_length = max(max([len(x) for x in train_sequences]), max([len(x) for x in test_sequences]))\n",
    "max_length = 200\n",
    "\n",
    "x_train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\")\n",
    "x_test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=\"post\")\n",
    "\n",
    "ytrain, ytest = np.asarray(df_train[\"sentiment\"]), np.asarray(df_test[\"sentiment\"])\n",
    "\n",
    "print(max_length)\n",
    "print(x_train_padded.shape)\n",
    "print(x_test_padded.shape)\n",
    "print(ytrain.shape)\n",
    "print(ytest.shape)\n",
    "print(f\"Found {len(word_index)} unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 200, 256)          58880000  \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 196, 128)          163968    \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 196, 128)          0         \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPooling  (None, 39, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 35, 128)           82048     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 35, 128)           0         \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPooling  (None, 7, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 3, 64)             41024     \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 3, 64)             0         \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 64)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 59,169,153\n",
      "Trainable params: 59,169,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(230000, 256, input_length=max_length))\n",
    "model.add(Conv1D(filters=128, kernel_size=5))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(filters=128, kernel_size=5))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(filters=64, kernel_size=5))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "DECAY_RATE = LEARNING_RATE / EPOCHS\n",
    "MOMENTUM = 0.1\n",
    "\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "# optimizer = SGD(learning_rate=LEARNING_RATE, momentum=MOMENTUM, decay=DECAY_RATE)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "history = model.fit(x_train_padded, ytrain, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
